{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICP5(JK).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OUEr8ztPfwz",
        "outputId": "59431211-57e0-4c54-d941-279c6a20dcec"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/67/5158f846202d7f012d1c9ca21c3549a58fd3c6707ae8ee823adcaca6473c/pyspark-3.0.2.tar.gz (204.8MB)\n",
            "\u001b[K     |████████████████████████████████| 204.8MB 80kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 55.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.2-py2.py3-none-any.whl size=205186687 sha256=49c0210da8da7e2db025cd1482f103cc62f902b9b9777f98afb4a7e046562bbb\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/09/da/c1f2859bcc86375dc972c5b6af4881b3603269bcc4c9be5d16\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gFSCm5PPf96"
      },
      "source": [
        "from __future__ import print_function\r\n",
        "from pyspark import SparkConf, SparkContext\r\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.ml.feature import NGram\r\n",
        "from pyspark.ml.feature import Word2Vec"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3RqI5W9PwEU"
      },
      "source": [
        "# creating spark session\r\n",
        "spark = SparkSession.builder.appName(\"TfIdf Example\").getOrCreate()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ynmbdljP37e"
      },
      "source": [
        "# creating spark dataframe wiht the input data. You can also read the data from file. label represents the 3 documnets (0.0,0.1,0.2)\r\n",
        "sentenceData = spark.createDataFrame([\r\n",
        "        (0.0, \"Welcome to KDM TF_IDF Tutorial.\"),\r\n",
        "        (0.1, \"Learn Spark ml tf_idf in today's lab.\"),\r\n",
        "        (0.2, \"Spark Mllib has TF-IDF.\")\r\n",
        "    ], [\"label\", \"sentence\"])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAuDw1JTP_j9"
      },
      "source": [
        "# creating tokens/words from the sentence data\r\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\r\n",
        "wordsData = tokenizer.transform(sentenceData)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juPnzf1HkBNi",
        "outputId": "49452d54-8409-42d6-edd0-4a920002715e"
      },
      "source": [
        "wordsData.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+\n",
            "|label|            sentence|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|  0.0|Welcome to KDM TF...|[welcome, to, kdm...|\n",
            "|  0.1|Learn Spark ml tf...|[learn, spark, ml...|\n",
            "|  0.2|Spark Mllib has T...|[spark, mllib, ha...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWWoLXIKQEp-"
      },
      "source": [
        "# applying tf on the words data\r\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\r\n",
        "featurizedData = hashingTF.transform(wordsData)\r\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqsyc3TFQLX4"
      },
      "source": [
        "# calculating the IDF\r\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\r\n",
        "idfModel = idf.fit(featurizedData)\r\n",
        "rescaledData = idfModel.transform(featurizedData)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ej95DLvjQS1T",
        "outputId": "93913a55-a3f9-4172-aa89-e01fe38f0932"
      },
      "source": [
        "#displaying the results\r\n",
        "rescaledData.select(\"label\", \"features\").show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(20,[2,8,13,15,17...|\n",
            "|  0.1|(20,[2,3,6,7],[0....|\n",
            "|  0.2|(20,[6,14,15],[0....|\n",
            "+-----+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFY0atcwQiCs"
      },
      "source": [
        "spark2 = SparkSession.builder.appName(\"Ngram Example\").getOrCreate()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5j1F-kR6Q2Ay"
      },
      "source": [
        "#creating dataframe of input\r\n",
        "wordDataFrame = spark2.createDataFrame([\r\n",
        "    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\r\n",
        "    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\r\n",
        "    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\r\n",
        "], [\"id\", \"words\"])\r\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xnz2_iOQ_xO"
      },
      "source": [
        "#creating NGrams with n=2 (two words)\r\n",
        "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\r\n",
        "ngramDataFrame = ngram.transform(wordDataFrame)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMhJK57bRGD5",
        "outputId": "611102f0-57a6-49c1-f8d7-85c6c9eaa6b1"
      },
      "source": [
        "# displaying the results\r\n",
        "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------------+\n",
            "|ngrams                                                            |\n",
            "+------------------------------------------------------------------+\n",
            "|[Hi I, I heard, heard about, about Spark]                         |\n",
            "|[I wish, wish Java, Java could, could use, use case, case classes]|\n",
            "|[Logistic regression, regression models, models are, are neat]    |\n",
            "+------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzQ7KVW3Riz0"
      },
      "source": [
        "# creating spark session\r\n",
        "spark3 = SparkSession.builder.appName(\"Word2Vec Example\").getOrCreate()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzHNuEa5Ri9E"
      },
      "source": [
        "# Input data: Each row is a bag of words from a sentence or document.\r\n",
        "documentDF = spark3.createDataFrame([\r\n",
        "    (\"McCarthy was asked to analyse the data from the first phase of trials of the vaccine.\".split(\" \"), ),\r\n",
        "    (\"We have amassed the raw data and are about to begin analysing it.\".split(\" \"), ),\r\n",
        "    (\"Without more data we cannot make a meaningful comparison of the two systems.\".split(\" \"), ),\r\n",
        "    (\"Collecting data is a painfully slow process.\".split(\" \"), ),\r\n",
        "    (\"You need a long series of data to be able to discern such a trend.\".split(\" \"), )\r\n",
        "], [\"text\"])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IjCO6rRRjCu"
      },
      "source": [
        "# Learn a mapping from words to Vectors.\r\n",
        "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\r\n",
        "model = word2Vec.fit(documentDF)\r\n",
        "result = model.transform(documentDF)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmVIfXsHSHUs",
        "outputId": "6a1ef847-de2e-4939-f0d2-02aa01f82ad1"
      },
      "source": [
        "for row in result.collect():\r\n",
        "    text, vector = row\r\n",
        "    #printing the results\r\n",
        "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text: [McCarthy, was, asked, to, analyse, the, data, from, the, first, phase, of, trials, of, the, vaccine.] => \n",
            "Vector: [-0.0035007820115424693,-0.0017171840299852192,0.02368130796821788]\n",
            "\n",
            "Text: [We, have, amassed, the, raw, data, and, are, about, to, begin, analysing, it.] => \n",
            "Vector: [-0.017147045116871595,-0.007818681689409109,0.03364002683128302]\n",
            "\n",
            "Text: [Without, more, data, we, cannot, make, a, meaningful, comparison, of, the, two, systems.] => \n",
            "Vector: [-0.01177951755324522,-0.032720054571445174,0.015373159880534962]\n",
            "\n",
            "Text: [Collecting, data, is, a, painfully, slow, process.] => \n",
            "Vector: [0.014085809966283185,0.001814738182084901,-0.0029639844516558306]\n",
            "\n",
            "Text: [You, need, a, long, series, of, data, to, be, able, to, discern, such, a, trend.] => \n",
            "Vector: [-0.00949504499634107,-0.012580770254135131,0.03007583270470301]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCbv_TB_SeqZ",
        "outputId": "5b6503c3-01fb-4fd5-fc5c-2a6d7e52ef56"
      },
      "source": [
        "# showing the synonyms and cosine similarity of the word in input data\r\n",
        "synonyms = model.findSynonyms(\"data\", 5)   # its okay for certain words , real bad for others\r\n",
        "synonyms.show(5)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+------------------+\n",
            "|     word|        similarity|\n",
            "+---------+------------------+\n",
            "|      was|0.9340987205505371|\n",
            "|      and|0.9194383025169373|\n",
            "|       be|0.9047830700874329|\n",
            "|analysing|0.8658599853515625|\n",
            "|      the|0.8481355905532837|\n",
            "+---------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtdspYI3Qg1o"
      },
      "source": [
        "#closing the spark sessions\r\n",
        "spark.stop()\r\n",
        "spark2.stop()\r\n",
        "spark3.stop()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0xeA1bchUFn"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkE7kKIdhYXL"
      },
      "source": [
        "# ICP5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MT7zXiCE_z54",
        "outputId": "05b3f17b-9fdc-4d4a-afef-5a138255291c"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iglRVjshT7C"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/text1.txt\",\"r+\") as t1:\r\n",
        "    doc1 = t1.read()\r\n",
        "with open(\"/content/drive/MyDrive/text2.txt\",\"r+\") as t2:\r\n",
        "    doc2 = t2.read()\r\n",
        "with open(\"/content/drive/MyDrive/text3.txt\",\"r+\") as t3:\r\n",
        "    doc3 = t3.read()\r\n",
        "with open(\"/content/drive/MyDrive/text4.txt\",\"r+\") as t4:\r\n",
        "    doc4 = t4.read()\r\n",
        "with open(\"/content/drive/MyDrive/text5.txt\",\"r+\") as t5:\r\n",
        "    doc5 = t5.read()\r\n",
        "# Read all 5 txt files in document list \r\n",
        "documents = [doc1,doc2,doc3,doc4,doc5]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9XjFPEahTzj"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_KbMed_5qxT"
      },
      "source": [
        "#a. Top 10 TF-IDF words for the above input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciC33FIdhTr7",
        "outputId": "b9cd3bd7-a4a8-415e-fd46-25bbb9429c37"
      },
      "source": [
        "\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "# using sklearn library which has inbuilt Tfidf vectorizer class which can generate tfidf for given corpus\r\n",
        "vect = TfidfVectorizer()\r\n",
        "#created TfidfVectorizer object\r\n",
        "tfidf_matrix = vect.fit_transform(documents)\r\n",
        "#passed list of documents or corpus to obt method fit_transform\r\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\r\n",
        "# converted method output to panda data frame \r\n",
        "pd.set_option('display.max_columns', 20)\r\n",
        "\r\n",
        "df.loc['Total'] = df.sum() # adding row to value total\r\n",
        "\r\n",
        "#  used transpose function here to filter out words (which was rows) and then converted matrix back to original version\r\n",
        "print (df.T.sort_values('Total', ascending=True).tail(10).T)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             in       him      fish       for       was        he       and  \\\n",
            "0      0.058153  0.000000  0.000000  0.068756  0.081733  0.000000  0.068756   \n",
            "1      0.062718  0.000000  0.000000  0.148305  0.000000  0.000000  0.148305   \n",
            "2      0.089482  0.062882  0.000000  0.000000  0.125764  0.000000  0.105797   \n",
            "3      0.048144  0.202994  0.000000  0.113843  0.202994  0.326059  0.170765   \n",
            "4      0.059707  0.083916  0.375903  0.070592  0.000000  0.101092  0.000000   \n",
            "Total  0.318204  0.349792  0.375903  0.401496  0.410491  0.427151  0.493623   \n",
            "\n",
            "             to        of       the  \n",
            "0      0.058153  0.408663  0.407074  \n",
            "1      0.062718  0.176295  0.439023  \n",
            "2      0.223706  0.000000  0.134224  \n",
            "3      0.096288  0.000000  0.144432  \n",
            "4      0.119413  0.167831  0.179120  \n",
            "Total  0.560278  0.752789  1.303872  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFYmq-MAhTck"
      },
      "source": [
        ""
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s8VqaUw5yYS"
      },
      "source": [
        "# b. Top 10 TF-IDF words for the lemmatized input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZh17c0w5xIG",
        "outputId": "b1e8f1e9-3fe5-4dec-fcce-d79dd6a8679a"
      },
      "source": [
        "import nltk;nltk.download('punkt');nltk.download('wordnet')\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "\r\n",
        "#tokenize each text\r\n",
        "words1 = nltk.word_tokenize(doc1)\r\n",
        "words2 = nltk.word_tokenize(doc2)\r\n",
        "words3 = nltk.word_tokenize(doc3)\r\n",
        "words4 = nltk.word_tokenize(doc4)\r\n",
        "words5 = nltk.word_tokenize(doc5)\r\n",
        "\r\n",
        "lemmatized_document1 = ' '.join([lemmatizer.lemmatize(w) for w in words1])\r\n",
        "lemmatized_document2 = ' '.join([lemmatizer.lemmatize(w) for w in words2])\r\n",
        "lemmatized_document3 = ' '.join([lemmatizer.lemmatize(w) for w in words3])\r\n",
        "lemmatized_document4 = ' '.join([lemmatizer.lemmatize(w) for w in words4])\r\n",
        "lemmatized_document5 = ' '.join([lemmatizer.lemmatize(w) for w in words5])\r\n",
        "\r\n",
        "#merging each textfiles\r\n",
        "documents = [lemmatized_document1,lemmatized_document2,lemmatized_document3,lemmatized_document4,lemmatized_document5]\r\n",
        "\r\n",
        "# using sklearn library which has inbuilt Tfidf vectorizer class which can generate tfidf for given corpus\r\n",
        "vect = TfidfVectorizer()\r\n",
        "#created TfidfVectorizer object\r\n",
        "tfidf_matrix = vect.fit_transform(documents)\r\n",
        "#passed list of documents or corpus to obt method fit_transform\r\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\r\n",
        "# converted method output to panda data frame \r\n",
        "\r\n",
        "df.loc['Total'] = df.sum() # adding row to value total\r\n",
        "\r\n",
        "# also used transpose function here to filter out words (which was rows) and then converted matrix back to original version\r\n",
        "print (df.T.sort_values('Total', ascending=True).tail(10).T)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "            him      fish       for        wa        he       dog       and  \\\n",
            "0      0.000000  0.000000  0.069778  0.082948  0.000000  0.000000  0.069778   \n",
            "1      0.000000  0.000000  0.148755  0.000000  0.000000  0.000000  0.148755   \n",
            "2      0.061075  0.000000  0.000000  0.122150  0.000000  0.294306  0.102757   \n",
            "3      0.201305  0.000000  0.112896  0.201305  0.323346  0.161673  0.169344   \n",
            "4      0.083916  0.375903  0.070592  0.000000  0.101092  0.000000  0.000000   \n",
            "Total  0.346296  0.375903  0.402022  0.406403  0.424438  0.455979  0.490634   \n",
            "\n",
            "             to        of       the  \n",
            "0      0.059018  0.414739  0.413126  \n",
            "1      0.062908  0.176830  0.440356  \n",
            "2      0.217278  0.000000  0.130367  \n",
            "3      0.095487  0.000000  0.143230  \n",
            "4      0.119413  0.167831  0.179120  \n",
            "Total  0.554104  0.759400  1.306199  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnkXyI0v524s"
      },
      "source": [
        "# c. Top 10 TF-IDF words for the n-gram based input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZUReCpg52nt",
        "outputId": "dce5e14b-d458-47cf-f719-9f6f7a71d0d2"
      },
      "source": [
        "# this function takes document and n int value to generate list of n grams\r\n",
        "def ngrams(input, n):\r\n",
        "    input = input.split(' ')\r\n",
        "    output = []\r\n",
        "    for i in range(len(input)-n+1):\r\n",
        "        output.append(input[i:i+n])\r\n",
        "    return output\r\n",
        "\r\n",
        "ngram_doc1 = ' '.join([' '.join(x) for x in ngrams(doc1, 3)])\r\n",
        "ngram_doc2 = ' '.join([' '.join(x) for x in ngrams(doc2, 3)])\r\n",
        "ngram_doc3 = ' '.join([' '.join(x) for x in ngrams(doc3, 3)])\r\n",
        "ngram_doc4 = ' '.join([' '.join(x) for x in ngrams(doc4, 3)])\r\n",
        "ngram_doc5 = ' '.join([' '.join(x) for x in ngrams(doc5, 3)])\r\n",
        "\r\n",
        "documents = [doc1,doc2,doc3,doc4,doc5]\r\n",
        "\r\n",
        "# using sklearn library \r\n",
        "vect = TfidfVectorizer( ngram_range=(3,3)) # TfidfVectorizer has inbuilt ngram kwarg which show tfidf for ngrams\r\n",
        "#created TfidfVectorizer object\r\n",
        "tfidf_matrix = vect.fit_transform(documents)\r\n",
        "#passed list of documents or corpus to obt method fit_transform\r\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\r\n",
        "\r\n",
        "df.loc['Total'] = df.sum() # adding row to value total\r\n",
        "\r\n",
        "# used transpose function here to filter out words and then converted matrix back to original version\r\n",
        "print (df.T.sort_values('Total', ascending=True).tail(10).T)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       boycott over the  the slogan one  never hosted the  the run up  \\\n",
            "0              0.000000        0.000000          0.000000    0.000000   \n",
            "1              0.132453        0.132453          0.132453    0.132453   \n",
            "2              0.000000        0.000000          0.000000    0.000000   \n",
            "3              0.000000        0.000000          0.000000    0.000000   \n",
            "4              0.000000        0.000000          0.000000    0.000000   \n",
            "Total          0.132453        0.132453          0.132453    0.132453   \n",
            "\n",
            "       the olympics before  over the country  the health of  \\\n",
            "0                 0.000000          0.000000       0.000000   \n",
            "1                 0.132453          0.132453       0.132453   \n",
            "2                 0.000000          0.000000       0.000000   \n",
            "3                 0.000000          0.000000       0.000000   \n",
            "4                 0.000000          0.000000       0.000000   \n",
            "Total             0.132453          0.132453       0.132453   \n",
            "\n",
            "       the country human  in the run  smog might affect  \n",
            "0               0.000000    0.000000           0.000000  \n",
            "1               0.132453    0.132453           0.132453  \n",
            "2               0.000000    0.000000           0.000000  \n",
            "3               0.000000    0.000000           0.000000  \n",
            "4               0.000000    0.000000           0.000000  \n",
            "Total           0.132453    0.132453           0.132453  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA2OpNxJ52Et"
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tLp8ijg6BDW"
      },
      "source": [
        "#Task2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLS09yo16AfQ",
        "outputId": "27b09ed0-249c-4654-8925-dde426d5bdf8"
      },
      "source": [
        "from __future__ import print_function\r\n",
        "from pyspark import SparkConf, SparkContext\r\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.ml.feature import NGram\r\n",
        "from pyspark.ml.feature import Word2Vec\r\n",
        "# creating spark session\r\n",
        "spark = SparkSession.builder.appName(\"TfIdf Example\").getOrCreate()\r\n",
        "\r\n",
        "documentData = spark.createDataFrame([\r\n",
        "        (0.0, doc1),\r\n",
        "        (0.1, doc2),\r\n",
        "        (0.2, doc3),\r\n",
        "        (0.3, doc4),\r\n",
        "        (0.5, doc5)\r\n",
        "    ], [\"label\", \"document\"])\r\n",
        "\r\n",
        "# creating tokens/words from the sentence data\r\n",
        "tokenizer = Tokenizer(inputCol=\"document\", outputCol=\"words\")\r\n",
        "wordsData = tokenizer.transform(documentData)\r\n",
        "print (documentData)\r\n",
        "wordsData.show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataFrame[label: double, document: string]\n",
            "+-----+--------------------+--------------------+\n",
            "|label|            document|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|  0.0|As the sound of f...|[as, the, sound, ...|\n",
            "|  0.1|China had never h...|[china, had, neve...|\n",
            "|  0.2|A homeless man ri...|[a, homeless, man...|\n",
            "|  0.3|Hamlin previously...|[hamlin, previous...|\n",
            "|  0.5|Andy Trust has be...|[andy, trust, has...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG3A1MpF8li4"
      },
      "source": [
        "# a. Try without NLP\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBsDIwve7uTD",
        "outputId": "fc305318-f397-4e9f-c0de-10cb90058c71"
      },
      "source": [
        "\r\n",
        "# applying tf on the words data\r\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=50)\r\n",
        "tf = hashingTF.transform(wordsData)\r\n",
        "\r\n",
        "# calculating the IDF\r\n",
        "tf.cache()\r\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\r\n",
        "idf = idf.fit(tf)\r\n",
        "tfidf = idf.transform(tf)\r\n",
        "#displaying the results\r\n",
        "tfidf.select(\"label\", \"features\").show()\r\n",
        "\r\n",
        "\r\n",
        "print(\"TFIDF without NLP:\")\r\n",
        "for row in tfidf.collect():\r\n",
        "    print(row)\r\n",
        "    print(row['rawFeatures'])\r\n",
        "spark.stop()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(50,[0,1,2,4,6,8,...|\n",
            "|  0.1|(50,[0,1,2,3,5,9,...|\n",
            "|  0.2|(50,[0,1,3,5,6,8,...|\n",
            "|  0.3|(50,[1,3,4,5,6,7,...|\n",
            "|  0.5|(50,[0,5,8,9,10,1...|\n",
            "+-----+--------------------+\n",
            "\n",
            "TFIDF without NLP:\n",
            "Row(label=0.0, document=\"As the sound of fireworks rang out over Beijing to mark the close of the 2008 Summer Olympics, China's leaders could have been forgiven for breathing a sigh of relief. Remembered today as an event in which record-breaking sporting achievements were matched only by the spectacular pageantry and organization of the Games, the success of the Beijing Olympics was no sure thing.\", words=['as', 'the', 'sound', 'of', 'fireworks', 'rang', 'out', 'over', 'beijing', 'to', 'mark', 'the', 'close', 'of', 'the', '2008', 'summer', 'olympics,', \"china's\", 'leaders', 'could', 'have', 'been', 'forgiven', 'for', 'breathing', 'a', 'sigh', 'of', 'relief.', 'remembered', 'today', 'as', 'an', 'event', 'in', 'which', 'record-breaking', 'sporting', 'achievements', 'were', 'matched', 'only', 'by', 'the', 'spectacular', 'pageantry', 'and', 'organization', 'of', 'the', 'games,', 'the', 'success', 'of', 'the', 'beijing', 'olympics', 'was', 'no', 'sure', 'thing.'], rawFeatures=SparseVector(50, {0: 2.0, 1: 1.0, 2: 1.0, 4: 3.0, 6: 1.0, 8: 1.0, 10: 1.0, 12: 1.0, 13: 2.0, 14: 1.0, 16: 1.0, 17: 10.0, 19: 1.0, 20: 1.0, 21: 1.0, 24: 1.0, 28: 2.0, 29: 1.0, 30: 1.0, 31: 3.0, 32: 1.0, 33: 1.0, 34: 1.0, 35: 3.0, 38: 2.0, 40: 2.0, 41: 2.0, 44: 1.0, 45: 6.0, 47: 2.0, 49: 5.0}), features=SparseVector(50, {0: 0.3646, 1: 0.1823, 2: 0.6931, 4: 2.0794, 6: 0.4055, 8: 0.1823, 10: 0.1823, 12: 0.0, 13: 0.0, 14: 0.0, 16: 0.1823, 17: 0.0, 19: 0.1823, 20: 0.1823, 21: 0.4055, 24: 0.4055, 28: 0.3646, 29: 0.1823, 30: 0.1823, 31: 1.2164, 32: 1.0986, 33: 0.4055, 34: 0.0, 35: 0.0, 38: 0.0, 40: 0.0, 41: 0.0, 44: 0.1823, 45: 0.0, 47: 0.8109, 49: 0.9116}))\n",
            "(50,[0,1,2,4,6,8,10,12,13,14,16,17,19,20,21,24,28,29,30,31,32,33,34,35,38,40,41,44,45,47,49],[2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,10.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,3.0,2.0,2.0,2.0,1.0,6.0,2.0,5.0])\n",
            "Row(label=0.1, document='China had never hosted the Olympics before, and in the run-up to the 2008 Games -- held under the slogan \"One World, One Dream\" -- there were calls for a boycott over the country\\'s human rights records, concerns for how Beijing\\'s notorious smog might affect the health of athletes, and angry pro-Tibet protests along much of the Olympic Torch relay.', words=['china', 'had', 'never', 'hosted', 'the', 'olympics', 'before,', 'and', 'in', 'the', 'run-up', 'to', 'the', '2008', 'games', '--', 'held', 'under', 'the', 'slogan', '\"one', 'world,', 'one', 'dream\"', '--', 'there', 'were', 'calls', 'for', 'a', 'boycott', 'over', 'the', \"country's\", 'human', 'rights', 'records,', 'concerns', 'for', 'how', \"beijing's\", 'notorious', 'smog', 'might', 'affect', 'the', 'health', 'of', 'athletes,', 'and', 'angry', 'pro-tibet', 'protests', 'along', 'much', 'of', 'the', 'olympic', 'torch', 'relay.'], rawFeatures=SparseVector(50, {0: 1.0, 1: 1.0, 2: 2.0, 3: 2.0, 5: 2.0, 9: 1.0, 12: 1.0, 13: 3.0, 14: 2.0, 15: 2.0, 17: 9.0, 18: 1.0, 19: 2.0, 20: 2.0, 23: 1.0, 24: 2.0, 26: 1.0, 27: 1.0, 31: 3.0, 34: 2.0, 35: 1.0, 38: 2.0, 39: 2.0, 40: 1.0, 41: 2.0, 42: 2.0, 43: 1.0, 44: 3.0, 45: 2.0, 46: 1.0, 48: 2.0}), features=SparseVector(50, {0: 0.1823, 1: 0.1823, 2: 1.3863, 3: 0.8109, 5: 0.3646, 9: 0.1823, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.8109, 17: 0.0, 18: 0.1823, 19: 0.3646, 20: 0.3646, 23: 0.4055, 24: 0.8109, 26: 0.1823, 27: 0.4055, 31: 1.2164, 34: 0.0, 35: 0.0, 38: 0.0, 39: 0.8109, 40: 0.0, 41: 0.0, 42: 0.8109, 43: 0.1823, 44: 0.547, 45: 0.0, 46: 0.1823, 48: 0.8109}))\n",
            "(50,[0,1,2,3,5,9,12,13,14,15,17,18,19,20,23,24,26,27,31,34,35,38,39,40,41,42,43,44,45,46,48],[1.0,1.0,2.0,2.0,2.0,1.0,1.0,3.0,2.0,2.0,9.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,3.0,2.0,1.0,2.0,2.0,1.0,2.0,2.0,1.0,3.0,2.0,1.0,2.0])\n",
            "Row(label=0.2, document='A homeless man risked his life to save several cats and dogs trapped at an Atlanta animal shelter after it caught fire, the facility\\'s founder said.Keith Walker, 53, rushed into the W-Underdogs shelter on December 18 after a fire engulfed its kitchen. \"I was nervous as hell, I\\'m not going to lie. I was really scared to go in there with all that smoke. But God put me there to save those animals,\" Walker told CNN. \"If you love a dog, you can love anyone in the world. My dog is my best friend, and I wouldn\\'t be here without him, so I knew I had to save all those other dogs.\"', words=['a', 'homeless', 'man', 'risked', 'his', 'life', 'to', 'save', 'several', 'cats', 'and', 'dogs', 'trapped', 'at', 'an', 'atlanta', 'animal', 'shelter', 'after', 'it', 'caught', 'fire,', 'the', \"facility's\", 'founder', 'said.keith', 'walker,', '53,', 'rushed', 'into', 'the', 'w-underdogs', 'shelter', 'on', 'december', '18', 'after', 'a', 'fire', 'engulfed', 'its', 'kitchen.', '\"i', 'was', 'nervous', 'as', 'hell,', \"i'm\", 'not', 'going', 'to', 'lie.', 'i', 'was', 'really', 'scared', 'to', 'go', 'in', 'there', 'with', 'all', 'that', 'smoke.', 'but', 'god', 'put', 'me', 'there', 'to', 'save', 'those', 'animals,\"', 'walker', 'told', 'cnn.', '\"if', 'you', 'love', 'a', 'dog,', 'you', 'can', 'love', 'anyone', 'in', 'the', 'world.', 'my', 'dog', 'is', 'my', 'best', 'friend,', 'and', 'i', \"wouldn't\", 'be', 'here', 'without', 'him,', 'so', 'i', 'knew', 'i', 'had', 'to', 'save', 'all', 'those', 'other', 'dogs.\"'], rawFeatures=SparseVector(50, {0: 3.0, 1: 3.0, 3: 1.0, 5: 4.0, 6: 6.0, 8: 1.0, 9: 2.0, 10: 3.0, 12: 3.0, 13: 6.0, 14: 4.0, 15: 1.0, 16: 3.0, 17: 8.0, 18: 3.0, 19: 4.0, 20: 3.0, 21: 2.0, 22: 3.0, 23: 2.0, 25: 1.0, 26: 1.0, 27: 2.0, 28: 1.0, 29: 2.0, 30: 1.0, 33: 3.0, 34: 1.0, 35: 1.0, 36: 3.0, 37: 3.0, 38: 8.0, 39: 4.0, 40: 5.0, 41: 5.0, 43: 1.0, 45: 1.0, 46: 1.0, 48: 2.0, 49: 1.0}), features=SparseVector(50, {0: 0.547, 1: 0.547, 3: 0.4055, 5: 0.7293, 6: 2.4328, 8: 0.1823, 9: 0.3646, 10: 0.547, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.4055, 16: 0.547, 17: 0.0, 18: 0.547, 19: 0.7293, 20: 0.547, 21: 0.8109, 22: 1.2164, 23: 0.8109, 25: 0.4055, 26: 0.1823, 27: 0.8109, 28: 0.1823, 29: 0.3646, 30: 0.1823, 33: 1.2164, 34: 0.0, 35: 0.0, 36: 1.2164, 37: 2.0794, 38: 0.0, 39: 1.6219, 40: 0.0, 41: 0.0, 43: 0.1823, 45: 0.0, 46: 0.1823, 48: 0.8109, 49: 0.1823}))\n",
            "(50,[0,1,3,5,6,8,9,10,12,13,14,15,16,17,18,19,20,21,22,23,25,26,27,28,29,30,33,34,35,36,37,38,39,40,41,43,45,46,48,49],[3.0,3.0,1.0,4.0,6.0,1.0,2.0,3.0,3.0,6.0,4.0,1.0,3.0,8.0,3.0,4.0,3.0,2.0,3.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,3.0,1.0,1.0,3.0,3.0,8.0,4.0,5.0,5.0,1.0,1.0,1.0,2.0,1.0])\n",
            "Row(label=0.3, document='Hamlin previously knew Walker, who has been homeless since he was 13 years old, because she lets him keep his dog, a pitbull named Bravo, at the shelter every night. Walker was on his way to pick up Bravo and take him for a walk when he saw the fire. He was able to rescue all the animals -- six dogs and 10 cats. \"I can\\'t thank him enough for saving my animals,\" Hamlin said. \"I\\'m still in disbelief, because I\\'ve been around a fire and I know how fast they flare up. He is my hero.\"', words=['hamlin', 'previously', 'knew', 'walker,', 'who', 'has', 'been', 'homeless', 'since', 'he', 'was', '13', 'years', 'old,', 'because', 'she', 'lets', 'him', 'keep', 'his', 'dog,', 'a', 'pitbull', 'named', 'bravo,', 'at', 'the', 'shelter', 'every', 'night.', 'walker', 'was', 'on', 'his', 'way', 'to', 'pick', 'up', 'bravo', 'and', 'take', 'him', 'for', 'a', 'walk', 'when', 'he', 'saw', 'the', 'fire.', 'he', 'was', 'able', 'to', 'rescue', 'all', 'the', 'animals', '--', 'six', 'dogs', 'and', '10', 'cats.', '\"i', \"can't\", 'thank', 'him', 'enough', 'for', 'saving', 'my', 'animals,\"', 'hamlin', 'said.', '\"i\\'m', 'still', 'in', 'disbelief,', 'because', \"i've\", 'been', 'around', 'a', 'fire', 'and', 'i', 'know', 'how', 'fast', 'they', 'flare', 'up.', 'he', 'is', 'my', 'hero.\"'], rawFeatures=SparseVector(50, {1: 2.0, 3: 1.0, 4: 1.0, 5: 3.0, 6: 3.0, 7: 2.0, 8: 1.0, 9: 5.0, 10: 1.0, 12: 3.0, 13: 5.0, 14: 1.0, 16: 1.0, 17: 7.0, 18: 2.0, 20: 2.0, 22: 3.0, 23: 1.0, 25: 1.0, 26: 3.0, 28: 2.0, 29: 1.0, 30: 1.0, 31: 3.0, 33: 3.0, 34: 4.0, 35: 2.0, 36: 2.0, 37: 2.0, 38: 2.0, 39: 2.0, 40: 2.0, 41: 5.0, 42: 2.0, 43: 1.0, 44: 3.0, 45: 1.0, 46: 2.0, 47: 3.0, 48: 5.0, 49: 1.0}), features=SparseVector(50, {1: 0.3646, 3: 0.4055, 4: 0.6931, 5: 0.547, 6: 1.2164, 7: 2.1972, 8: 0.1823, 9: 0.9116, 10: 0.1823, 12: 0.0, 13: 0.0, 14: 0.0, 16: 0.1823, 17: 0.0, 18: 0.3646, 20: 0.3646, 22: 1.2164, 23: 0.4055, 25: 0.4055, 26: 0.547, 28: 0.3646, 29: 0.1823, 30: 0.1823, 31: 1.2164, 33: 1.2164, 34: 0.0, 35: 0.0, 36: 0.8109, 37: 1.3863, 38: 0.0, 39: 0.8109, 40: 0.0, 41: 0.0, 42: 0.8109, 43: 0.1823, 44: 0.547, 45: 0.0, 46: 0.3646, 47: 1.2164, 48: 2.0273, 49: 0.1823}))\n",
            "(50,[1,3,4,5,6,7,8,9,10,12,13,14,16,17,18,20,22,23,25,26,28,29,30,31,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],[2.0,1.0,1.0,3.0,3.0,2.0,1.0,5.0,1.0,3.0,5.0,1.0,1.0,7.0,2.0,2.0,3.0,1.0,1.0,3.0,2.0,1.0,1.0,3.0,3.0,4.0,2.0,2.0,2.0,2.0,2.0,2.0,5.0,2.0,1.0,3.0,1.0,2.0,3.0,5.0,1.0])\n",
            "Row(label=0.5, document='Andy Trust has been exporting fish from Cornwall to continental Europe for 20 years. However, the past seven weeks have given him cause to contemplate shutting down the entire European operation of his fish merchants, Ocean Harvest. \"The cost of sending fish into Europe has more than trebled. In an industry that operates on thin profit margins, it could destroy British fishing,\" he says.', words=['andy', 'trust', 'has', 'been', 'exporting', 'fish', 'from', 'cornwall', 'to', 'continental', 'europe', 'for', '20', 'years.', 'however,', 'the', 'past', 'seven', 'weeks', 'have', 'given', 'him', 'cause', 'to', 'contemplate', 'shutting', 'down', 'the', 'entire', 'european', 'operation', 'of', 'his', 'fish', 'merchants,', 'ocean', 'harvest.', '\"the', 'cost', 'of', 'sending', 'fish', 'into', 'europe', 'has', 'more', 'than', 'trebled.', 'in', 'an', 'industry', 'that', 'operates', 'on', 'thin', 'profit', 'margins,', 'it', 'could', 'destroy', 'british', 'fishing,\"', 'he', 'says.'], rawFeatures=SparseVector(50, {0: 1.0, 5: 1.0, 8: 1.0, 9: 1.0, 10: 2.0, 11: 3.0, 12: 2.0, 13: 2.0, 14: 2.0, 15: 2.0, 16: 3.0, 17: 4.0, 18: 1.0, 19: 1.0, 21: 2.0, 22: 1.0, 24: 1.0, 25: 2.0, 26: 3.0, 27: 2.0, 28: 1.0, 29: 1.0, 30: 1.0, 34: 2.0, 35: 2.0, 36: 3.0, 38: 3.0, 40: 3.0, 41: 1.0, 42: 1.0, 43: 1.0, 44: 1.0, 45: 3.0, 46: 2.0, 47: 1.0, 49: 1.0}), features=SparseVector(50, {0: 0.1823, 5: 0.1823, 8: 0.1823, 9: 0.1823, 10: 0.3646, 11: 3.2958, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.8109, 16: 0.547, 17: 0.0, 18: 0.1823, 19: 0.1823, 21: 0.8109, 22: 0.4055, 24: 0.4055, 25: 0.8109, 26: 0.547, 27: 0.8109, 28: 0.1823, 29: 0.1823, 30: 0.1823, 34: 0.0, 35: 0.0, 36: 1.2164, 38: 0.0, 40: 0.0, 41: 0.0, 42: 0.4055, 43: 0.1823, 44: 0.1823, 45: 0.0, 46: 0.3646, 47: 0.4055, 49: 0.1823}))\n",
            "(50,[0,5,8,9,10,11,12,13,14,15,16,17,18,19,21,22,24,25,26,27,28,29,30,34,35,36,38,40,41,42,43,44,45,46,47,49],[1.0,1.0,1.0,1.0,2.0,3.0,2.0,2.0,2.0,2.0,3.0,4.0,1.0,1.0,2.0,1.0,1.0,2.0,3.0,2.0,1.0,1.0,1.0,2.0,2.0,3.0,3.0,3.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k5JvNix9ryD"
      },
      "source": [
        ""
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX3w9cfH8dvK"
      },
      "source": [
        "# b. Try with Lemmatization\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YysjIm27yN4",
        "outputId": "fc70a851-6bdb-41ea-eb78-04b49550a070"
      },
      "source": [
        "\r\n",
        "import nltk;nltk.download('punkt');nltk.download('wordnet')\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "\r\n",
        "words1 = nltk.word_tokenize(doc1)\r\n",
        "words2 = nltk.word_tokenize(doc2)\r\n",
        "words3 = nltk.word_tokenize(doc3)\r\n",
        "words4 = nltk.word_tokenize(doc4)\r\n",
        "words5 = nltk.word_tokenize(doc5)\r\n",
        "\r\n",
        "lemmatized_document1 = ' '.join([lemmatizer.lemmatize(w) for w in words1])\r\n",
        "lemmatized_document2 = ' '.join([lemmatizer.lemmatize(w) for w in words2])\r\n",
        "lemmatized_document3 = ' '.join([lemmatizer.lemmatize(w) for w in words3])\r\n",
        "lemmatized_document4 = ' '.join([lemmatizer.lemmatize(w) for w in words4])\r\n",
        "lemmatized_document5 = ' '.join([lemmatizer.lemmatize(w) for w in words5])\r\n",
        "\r\n",
        "### lemmatizing words from 5 input docs same as previos task\r\n",
        "\r\n",
        "# creating spark session\r\n",
        "spark = SparkSession.builder.appName(\"TfIdf Example\").getOrCreate()\r\n",
        "\r\n",
        "documentData = spark.createDataFrame([\r\n",
        "        (0.0, lemmatized_document1),\r\n",
        "        (0.1, lemmatized_document2),\r\n",
        "        (0.2, lemmatized_document3),\r\n",
        "        (0.3, lemmatized_document4),\r\n",
        "        (0.5, lemmatized_document5)\r\n",
        "    ], [\"label\", \"document\"])\r\n",
        "\r\n",
        "# creating tokens/words from the sentence data\r\n",
        "tokenizer = Tokenizer(inputCol=\"document\", outputCol=\"words\")\r\n",
        "wordsData = tokenizer.transform(documentData)\r\n",
        "print (documentData)\r\n",
        "wordsData.show()\r\n",
        "\r\n",
        "\r\n",
        "# applying tf on the words data\r\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=50)\r\n",
        "tf = hashingTF.transform(wordsData)\r\n",
        "# calculating the IDF\r\n",
        "tf.cache()\r\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\r\n",
        "idf = idf.fit(tf)\r\n",
        "tfidf = idf.transform(tf)\r\n",
        "#displaying the results\r\n",
        "tfidf.select(\"label\", \"features\").show()\r\n",
        "\r\n",
        "\r\n",
        "print(\"TF-IDF with Lemmatization:\")\r\n",
        "for row in tfidf.collect():\r\n",
        "    print(row)\r\n",
        "    print(row['rawFeatures'])\r\n",
        "spark.stop()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "DataFrame[label: double, document: string]\n",
            "+-----+--------------------+--------------------+\n",
            "|label|            document|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|  0.0|As the sound of f...|[as, the, sound, ...|\n",
            "|  0.1|China had never h...|[china, had, neve...|\n",
            "|  0.2|A homeless man ri...|[a, homeless, man...|\n",
            "|  0.3|Hamlin previously...|[hamlin, previous...|\n",
            "|  0.5|Andy Trust ha bee...|[andy, trust, ha,...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n",
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(50,[0,2,4,6,7,8,...|\n",
            "|  0.1|(50,[0,1,2,3,5,7,...|\n",
            "|  0.2|(50,[0,1,3,4,5,6,...|\n",
            "|  0.3|(50,[1,3,4,5,6,7,...|\n",
            "|  0.5|(50,[0,2,4,7,9,10...|\n",
            "+-----+--------------------+\n",
            "\n",
            "TF-IDF with Lemmatization:\n",
            "Row(label=0.0, document=\"As the sound of firework rang out over Beijing to mark the close of the 2008 Summer Olympics , China 's leader could have been forgiven for breathing a sigh of relief . Remembered today a an event in which record-breaking sporting achievement were matched only by the spectacular pageantry and organization of the Games , the success of the Beijing Olympics wa no sure thing .\", words=['as', 'the', 'sound', 'of', 'firework', 'rang', 'out', 'over', 'beijing', 'to', 'mark', 'the', 'close', 'of', 'the', '2008', 'summer', 'olympics', ',', 'china', \"'s\", 'leader', 'could', 'have', 'been', 'forgiven', 'for', 'breathing', 'a', 'sigh', 'of', 'relief', '.', 'remembered', 'today', 'a', 'an', 'event', 'in', 'which', 'record-breaking', 'sporting', 'achievement', 'were', 'matched', 'only', 'by', 'the', 'spectacular', 'pageantry', 'and', 'organization', 'of', 'the', 'games', ',', 'the', 'success', 'of', 'the', 'beijing', 'olympics', 'wa', 'no', 'sure', 'thing', '.'], rawFeatures=SparseVector(50, {0: 1.0, 2: 1.0, 4: 3.0, 6: 1.0, 7: 1.0, 8: 1.0, 10: 1.0, 12: 1.0, 13: 3.0, 14: 2.0, 16: 1.0, 17: 13.0, 19: 1.0, 20: 1.0, 21: 2.0, 22: 1.0, 24: 1.0, 26: 1.0, 28: 4.0, 30: 1.0, 31: 3.0, 34: 1.0, 35: 3.0, 38: 3.0, 40: 1.0, 41: 2.0, 42: 1.0, 44: 2.0, 45: 5.0, 47: 2.0, 49: 3.0}), features=SparseVector(50, {0: 0.1823, 2: 0.4055, 4: 0.547, 6: 0.4055, 7: 0.0, 8: 1.0986, 10: 0.0, 12: 0.0, 13: 0.0, 14: 0.3646, 16: 0.0, 17: 0.0, 19: 0.0, 20: 0.1823, 21: 0.8109, 22: 0.1823, 24: 0.1823, 26: 0.0, 28: 0.0, 30: 0.1823, 31: 0.547, 34: 0.0, 35: 0.547, 38: 0.0, 40: 0.0, 41: 0.0, 42: 0.1823, 44: 0.3646, 45: 0.0, 47: 0.8109, 49: 1.2164}))\n",
            "(50,[0,2,4,6,7,8,10,12,13,14,16,17,19,20,21,22,24,26,28,30,31,34,35,38,40,41,42,44,45,47,49],[1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,13.0,1.0,1.0,2.0,1.0,1.0,1.0,4.0,1.0,3.0,1.0,3.0,3.0,1.0,2.0,1.0,2.0,5.0,2.0,3.0])\n",
            "Row(label=0.1, document=\"China had never hosted the Olympics before , and in the run-up to the 2008 Games -- held under the slogan `` One World , One Dream '' -- there were call for a boycott over the country 's human right record , concern for how Beijing 's notorious smog might affect the health of athlete , and angry pro-Tibet protest along much of the Olympic Torch relay .\", words=['china', 'had', 'never', 'hosted', 'the', 'olympics', 'before', ',', 'and', 'in', 'the', 'run-up', 'to', 'the', '2008', 'games', '--', 'held', 'under', 'the', 'slogan', '``', 'one', 'world', ',', 'one', 'dream', \"''\", '--', 'there', 'were', 'call', 'for', 'a', 'boycott', 'over', 'the', 'country', \"'s\", 'human', 'right', 'record', ',', 'concern', 'for', 'how', 'beijing', \"'s\", 'notorious', 'smog', 'might', 'affect', 'the', 'health', 'of', 'athlete', ',', 'and', 'angry', 'pro-tibet', 'protest', 'along', 'much', 'of', 'the', 'olympic', 'torch', 'relay', '.'], rawFeatures=SparseVector(50, {0: 1.0, 1: 1.0, 2: 2.0, 3: 1.0, 5: 2.0, 7: 1.0, 9: 1.0, 10: 1.0, 12: 1.0, 13: 3.0, 14: 2.0, 15: 2.0, 16: 2.0, 17: 13.0, 18: 2.0, 19: 2.0, 20: 3.0, 23: 1.0, 24: 2.0, 26: 1.0, 27: 1.0, 28: 1.0, 31: 2.0, 34: 2.0, 38: 3.0, 39: 2.0, 40: 2.0, 41: 2.0, 42: 2.0, 43: 1.0, 44: 2.0, 45: 2.0, 46: 1.0, 47: 1.0, 48: 1.0}), features=SparseVector(50, {0: 0.1823, 1: 0.4055, 2: 0.8109, 3: 0.4055, 5: 0.8109, 7: 0.0, 9: 0.1823, 10: 0.0, 12: 0.0, 13: 0.0, 14: 0.3646, 15: 1.3863, 16: 0.0, 17: 0.0, 18: 0.3646, 19: 0.0, 20: 0.547, 23: 1.0986, 24: 0.3646, 26: 0.0, 27: 0.4055, 28: 0.0, 31: 0.3646, 34: 0.0, 38: 0.0, 39: 0.3646, 40: 0.0, 41: 0.0, 42: 0.3646, 43: 0.1823, 44: 0.3646, 45: 0.0, 46: 0.1823, 47: 0.4055, 48: 0.4055}))\n",
            "(50,[0,1,2,3,5,7,9,10,12,13,14,15,16,17,18,19,20,23,24,26,27,28,31,34,38,39,40,41,42,43,44,45,46,47,48],[1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,2.0,2.0,2.0,13.0,2.0,2.0,3.0,1.0,2.0,1.0,1.0,1.0,2.0,2.0,3.0,2.0,2.0,2.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0])\n",
            "Row(label=0.2, document=\"A homeless man risked his life to save several cat and dog trapped at an Atlanta animal shelter after it caught fire , the facility 's founder said.Keith Walker , 53 , rushed into the W-Underdogs shelter on December 18 after a fire engulfed it kitchen . `` I wa nervous a hell , I 'm not going to lie . I wa really scared to go in there with all that smoke . But God put me there to save those animal , '' Walker told CNN . `` If you love a dog , you can love anyone in the world . My dog is my best friend , and I would n't be here without him , so I knew I had to save all those other dog . ''\", words=['a', 'homeless', 'man', 'risked', 'his', 'life', 'to', 'save', 'several', 'cat', 'and', 'dog', 'trapped', 'at', 'an', 'atlanta', 'animal', 'shelter', 'after', 'it', 'caught', 'fire', ',', 'the', 'facility', \"'s\", 'founder', 'said.keith', 'walker', ',', '53', ',', 'rushed', 'into', 'the', 'w-underdogs', 'shelter', 'on', 'december', '18', 'after', 'a', 'fire', 'engulfed', 'it', 'kitchen', '.', '``', 'i', 'wa', 'nervous', 'a', 'hell', ',', 'i', \"'m\", 'not', 'going', 'to', 'lie', '.', 'i', 'wa', 'really', 'scared', 'to', 'go', 'in', 'there', 'with', 'all', 'that', 'smoke', '.', 'but', 'god', 'put', 'me', 'there', 'to', 'save', 'those', 'animal', ',', \"''\", 'walker', 'told', 'cnn', '.', '``', 'if', 'you', 'love', 'a', 'dog', ',', 'you', 'can', 'love', 'anyone', 'in', 'the', 'world', '.', 'my', 'dog', 'is', 'my', 'best', 'friend', ',', 'and', 'i', 'would', \"n't\", 'be', 'here', 'without', 'him', ',', 'so', 'i', 'knew', 'i', 'had', 'to', 'save', 'all', 'those', 'other', 'dog', '.', \"''\"], rawFeatures=SparseVector(50, {0: 3.0, 1: 3.0, 3: 1.0, 4: 1.0, 5: 3.0, 6: 9.0, 7: 2.0, 9: 1.0, 10: 5.0, 11: 1.0, 12: 3.0, 13: 5.0, 14: 8.0, 16: 3.0, 17: 16.0, 18: 2.0, 19: 4.0, 20: 4.0, 21: 2.0, 22: 2.0, 24: 2.0, 26: 1.0, 27: 2.0, 28: 7.0, 29: 2.0, 30: 2.0, 31: 1.0, 33: 1.0, 34: 2.0, 35: 3.0, 36: 3.0, 37: 2.0, 38: 10.0, 39: 3.0, 40: 5.0, 41: 5.0, 43: 1.0, 45: 1.0, 46: 1.0, 48: 1.0}), features=SparseVector(50, {0: 0.547, 1: 1.2164, 3: 0.4055, 4: 0.1823, 5: 1.2164, 6: 3.6492, 7: 0.0, 9: 0.1823, 10: 0.0, 11: 0.6931, 12: 0.0, 13: 0.0, 14: 1.4586, 16: 0.0, 17: 0.0, 18: 0.3646, 19: 0.0, 20: 0.7293, 21: 0.8109, 22: 0.3646, 24: 0.3646, 26: 0.0, 27: 0.8109, 28: 0.0, 29: 0.8109, 30: 0.3646, 31: 0.1823, 33: 0.6931, 34: 0.0, 35: 0.547, 36: 1.2164, 37: 1.3863, 38: 0.0, 39: 0.547, 40: 0.0, 41: 0.0, 43: 0.1823, 45: 0.0, 46: 0.1823, 48: 0.4055}))\n",
            "(50,[0,1,3,4,5,6,7,9,10,11,12,13,14,16,17,18,19,20,21,22,24,26,27,28,29,30,31,33,34,35,36,37,38,39,40,41,43,45,46,48],[3.0,3.0,1.0,1.0,3.0,9.0,2.0,1.0,5.0,1.0,3.0,5.0,8.0,3.0,16.0,2.0,4.0,4.0,2.0,2.0,2.0,1.0,2.0,7.0,2.0,2.0,1.0,1.0,2.0,3.0,3.0,2.0,10.0,3.0,5.0,5.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.3, document=\"Hamlin previously knew Walker , who ha been homeless since he wa 13 year old , because she let him keep his dog , a pitbull named Bravo , at the shelter every night . Walker wa on his way to pick up Bravo and take him for a walk when he saw the fire . He wa able to rescue all the animal -- six dog and 10 cat . `` I ca n't thank him enough for saving my animal , '' Hamlin said . `` I 'm still in disbelief , because I 've been around a fire and I know how fast they flare up . He is my hero . ''\", words=['hamlin', 'previously', 'knew', 'walker', ',', 'who', 'ha', 'been', 'homeless', 'since', 'he', 'wa', '13', 'year', 'old', ',', 'because', 'she', 'let', 'him', 'keep', 'his', 'dog', ',', 'a', 'pitbull', 'named', 'bravo', ',', 'at', 'the', 'shelter', 'every', 'night', '.', 'walker', 'wa', 'on', 'his', 'way', 'to', 'pick', 'up', 'bravo', 'and', 'take', 'him', 'for', 'a', 'walk', 'when', 'he', 'saw', 'the', 'fire', '.', 'he', 'wa', 'able', 'to', 'rescue', 'all', 'the', 'animal', '--', 'six', 'dog', 'and', '10', 'cat', '.', '``', 'i', 'ca', \"n't\", 'thank', 'him', 'enough', 'for', 'saving', 'my', 'animal', ',', \"''\", 'hamlin', 'said', '.', '``', 'i', \"'m\", 'still', 'in', 'disbelief', ',', 'because', 'i', \"'ve\", 'been', 'around', 'a', 'fire', 'and', 'i', 'know', 'how', 'fast', 'they', 'flare', 'up', '.', 'he', 'is', 'my', 'hero', '.', \"''\"], rawFeatures=SparseVector(50, {1: 4.0, 3: 1.0, 4: 2.0, 5: 3.0, 6: 5.0, 7: 5.0, 9: 5.0, 10: 2.0, 12: 3.0, 13: 3.0, 14: 4.0, 16: 2.0, 17: 13.0, 18: 2.0, 19: 1.0, 20: 3.0, 22: 2.0, 25: 1.0, 26: 2.0, 28: 8.0, 29: 1.0, 30: 1.0, 31: 3.0, 33: 1.0, 34: 3.0, 35: 5.0, 36: 2.0, 37: 2.0, 38: 2.0, 39: 1.0, 40: 2.0, 41: 6.0, 42: 1.0, 43: 1.0, 44: 3.0, 45: 1.0, 46: 2.0, 47: 1.0, 48: 4.0, 49: 3.0}), features=SparseVector(50, {1: 1.6219, 3: 0.4055, 4: 0.3646, 5: 1.2164, 6: 2.0273, 7: 0.0, 9: 0.9116, 10: 0.0, 12: 0.0, 13: 0.0, 14: 0.7293, 16: 0.0, 17: 0.0, 18: 0.3646, 19: 0.0, 20: 0.547, 22: 0.3646, 25: 0.6931, 26: 0.0, 28: 0.0, 29: 0.4055, 30: 0.1823, 31: 0.547, 33: 0.6931, 34: 0.0, 35: 0.9116, 36: 0.8109, 37: 1.3863, 38: 0.0, 39: 0.1823, 40: 0.0, 41: 0.0, 42: 0.1823, 43: 0.1823, 44: 0.547, 45: 0.0, 46: 0.3646, 47: 0.4055, 48: 1.6219, 49: 1.2164}))\n",
            "(50,[1,3,4,5,6,7,9,10,12,13,14,16,17,18,19,20,22,25,26,28,29,30,31,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],[4.0,1.0,2.0,3.0,5.0,5.0,5.0,2.0,3.0,3.0,4.0,2.0,13.0,2.0,1.0,3.0,2.0,1.0,2.0,8.0,1.0,1.0,3.0,1.0,3.0,5.0,2.0,2.0,2.0,1.0,2.0,6.0,1.0,1.0,3.0,1.0,2.0,1.0,4.0,3.0])\n",
            "Row(label=0.5, document=\"Andy Trust ha been exporting fish from Cornwall to continental Europe for 20 year . However , the past seven week have given him cause to contemplate shutting down the entire European operation of his fish merchant , Ocean Harvest . `` The cost of sending fish into Europe ha more than trebled . In an industry that operates on thin profit margin , it could destroy British fishing , '' he say .\", words=['andy', 'trust', 'ha', 'been', 'exporting', 'fish', 'from', 'cornwall', 'to', 'continental', 'europe', 'for', '20', 'year', '.', 'however', ',', 'the', 'past', 'seven', 'week', 'have', 'given', 'him', 'cause', 'to', 'contemplate', 'shutting', 'down', 'the', 'entire', 'european', 'operation', 'of', 'his', 'fish', 'merchant', ',', 'ocean', 'harvest', '.', '``', 'the', 'cost', 'of', 'sending', 'fish', 'into', 'europe', 'ha', 'more', 'than', 'trebled', '.', 'in', 'an', 'industry', 'that', 'operates', 'on', 'thin', 'profit', 'margin', ',', 'it', 'could', 'destroy', 'british', 'fishing', ',', \"''\", 'he', 'say', '.'], rawFeatures=SparseVector(50, {0: 1.0, 2: 1.0, 4: 2.0, 7: 1.0, 9: 2.0, 10: 2.0, 11: 4.0, 12: 3.0, 13: 2.0, 15: 2.0, 16: 2.0, 17: 9.0, 18: 1.0, 19: 1.0, 21: 3.0, 22: 2.0, 24: 1.0, 25: 2.0, 26: 3.0, 27: 1.0, 28: 4.0, 29: 1.0, 30: 1.0, 34: 2.0, 35: 1.0, 36: 3.0, 38: 3.0, 39: 1.0, 40: 3.0, 41: 1.0, 42: 1.0, 43: 1.0, 44: 1.0, 45: 2.0, 46: 1.0, 49: 3.0}), features=SparseVector(50, {0: 0.1823, 2: 0.4055, 4: 0.3646, 7: 0.0, 9: 0.3646, 10: 0.0, 11: 2.7726, 12: 0.0, 13: 0.0, 15: 1.3863, 16: 0.0, 17: 0.0, 18: 0.1823, 19: 0.0, 21: 1.2164, 22: 0.3646, 24: 0.1823, 25: 1.3863, 26: 0.0, 27: 0.4055, 28: 0.0, 29: 0.4055, 30: 0.1823, 34: 0.0, 35: 0.1823, 36: 1.2164, 38: 0.0, 39: 0.1823, 40: 0.0, 41: 0.0, 42: 0.1823, 43: 0.1823, 44: 0.1823, 45: 0.0, 46: 0.1823, 49: 1.2164}))\n",
            "(50,[0,2,4,7,9,10,11,12,13,15,16,17,18,19,21,22,24,25,26,27,28,29,30,34,35,36,38,39,40,41,42,43,44,45,46,49],[1.0,1.0,2.0,1.0,2.0,2.0,4.0,3.0,2.0,2.0,2.0,9.0,1.0,1.0,3.0,2.0,1.0,2.0,3.0,1.0,4.0,1.0,1.0,2.0,1.0,3.0,3.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMa-dft276yB"
      },
      "source": [
        "# c. Try with N grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGD7ls5z75Ku",
        "outputId": "455d43d7-78b7-4460-d650-f217657538bc"
      },
      "source": [
        "spark = SparkSession.builder.appName(\"TfIdf Example\").getOrCreate()\r\n",
        "\r\n",
        "documentData = spark.createDataFrame([\r\n",
        "        (0.0, doc1.split(' ')),\r\n",
        "        (0.1, doc2.split(' ')),\r\n",
        "        (0.2, doc3.split(' ')),\r\n",
        "        (0.3, doc4.split(' ')),\r\n",
        "        (0.5, doc5.split(' '))\r\n",
        "    ], [\"label\", \"document\"])\r\n",
        "\r\n",
        "\r\n",
        "# Using ngram \r\n",
        "ngram = NGram(n=2, inputCol=\"document\", outputCol=\"ngrams\")\r\n",
        "\r\n",
        "ngramDataFrame = ngram.transform(documentData)\r\n",
        "\r\n",
        "# applying tf on the words data\r\n",
        "hashingTF = HashingTF(inputCol=\"ngrams\", outputCol=\"rawFeatures\", numFeatures=50)\r\n",
        "tf = hashingTF.transform(ngramDataFrame)\r\n",
        "# calculating the IDF\r\n",
        "tf.cache()\r\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\r\n",
        "idf = idf.fit(tf)\r\n",
        "tfidf = idf.transform(tf)\r\n",
        "#displaying the results\r\n",
        "tfidf.select(\"label\", \"features\").show()\r\n",
        "\r\n",
        "\r\n",
        "print(\"TF-IDF with ngram:\")\r\n",
        "for row in tfidf.collect():\r\n",
        "    print(row)\r\n",
        "    print(row['rawFeatures'])\r\n",
        "spark.stop()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(50,[0,1,4,5,6,7,...|\n",
            "|  0.1|(50,[0,1,2,3,4,5,...|\n",
            "|  0.2|(50,[0,1,2,4,5,6,...|\n",
            "|  0.3|(50,[0,2,3,4,5,7,...|\n",
            "|  0.5|(50,[0,1,2,4,6,8,...|\n",
            "+-----+--------------------+\n",
            "\n",
            "TF-IDF with ngram:\n",
            "Row(label=0.0, document=['As', 'the', 'sound', 'of', 'fireworks', 'rang', 'out', 'over', 'Beijing', 'to', 'mark', 'the', 'close', 'of', 'the', '2008', 'Summer', 'Olympics,', \"China's\", 'leaders', 'could', 'have', 'been', 'forgiven', 'for', 'breathing', 'a', 'sigh', 'of', 'relief.', 'Remembered', 'today', 'as', 'an', 'event', 'in', 'which', 'record-breaking', 'sporting', 'achievements', 'were', 'matched', 'only', 'by', 'the', 'spectacular', 'pageantry', 'and', 'organization', 'of', 'the', 'Games,', 'the', 'success', 'of', 'the', 'Beijing', 'Olympics', 'was', 'no', 'sure', 'thing.'], ngrams=['As the', 'the sound', 'sound of', 'of fireworks', 'fireworks rang', 'rang out', 'out over', 'over Beijing', 'Beijing to', 'to mark', 'mark the', 'the close', 'close of', 'of the', 'the 2008', '2008 Summer', 'Summer Olympics,', \"Olympics, China's\", \"China's leaders\", 'leaders could', 'could have', 'have been', 'been forgiven', 'forgiven for', 'for breathing', 'breathing a', 'a sigh', 'sigh of', 'of relief.', 'relief. Remembered', 'Remembered today', 'today as', 'as an', 'an event', 'event in', 'in which', 'which record-breaking', 'record-breaking sporting', 'sporting achievements', 'achievements were', 'were matched', 'matched only', 'only by', 'by the', 'the spectacular', 'spectacular pageantry', 'pageantry and', 'and organization', 'organization of', 'of the', 'the Games,', 'Games, the', 'the success', 'success of', 'of the', 'the Beijing', 'Beijing Olympics', 'Olympics was', 'was no', 'no sure', 'sure thing.'], rawFeatures=SparseVector(50, {0: 2.0, 1: 1.0, 4: 1.0, 5: 3.0, 6: 4.0, 7: 2.0, 8: 1.0, 10: 1.0, 11: 1.0, 13: 1.0, 14: 1.0, 16: 2.0, 18: 1.0, 19: 1.0, 22: 2.0, 23: 2.0, 25: 1.0, 27: 3.0, 28: 1.0, 29: 2.0, 31: 2.0, 33: 1.0, 34: 1.0, 35: 1.0, 36: 2.0, 38: 1.0, 39: 3.0, 40: 2.0, 41: 2.0, 42: 1.0, 43: 1.0, 44: 2.0, 45: 2.0, 46: 2.0, 47: 2.0, 48: 3.0}), features=SparseVector(50, {0: 0.0, 1: 0.1823, 4: 0.0, 5: 0.547, 6: 0.7293, 7: 0.8109, 8: 0.1823, 10: 0.0, 11: 0.1823, 13: 0.4055, 14: 0.4055, 16: 0.0, 18: 0.0, 19: 0.4055, 22: 0.0, 23: 0.0, 25: 0.4055, 27: 0.0, 28: 0.0, 29: 0.3646, 31: 0.0, 33: 0.1823, 34: 0.6931, 35: 0.1823, 36: 0.0, 38: 0.1823, 39: 1.2164, 40: 0.0, 41: 0.0, 42: 0.0, 43: 0.1823, 44: 0.3646, 45: 0.8109, 46: 0.3646, 47: 0.3646, 48: 0.0}))\n",
            "(50,[0,1,4,5,6,7,8,10,11,13,14,16,18,19,22,23,25,27,28,29,31,33,34,35,36,38,39,40,41,42,43,44,45,46,47,48],[2.0,1.0,1.0,3.0,4.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,3.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,3.0,2.0,2.0,1.0,1.0,2.0,2.0,2.0,2.0,3.0])\n",
            "Row(label=0.1, document=['China', 'had', 'never', 'hosted', 'the', 'Olympics', 'before,', 'and', 'in', 'the', 'run-up', 'to', 'the', '2008', 'Games', '--', 'held', 'under', 'the', 'slogan', '\"One', 'World,', 'One', 'Dream\"', '--', 'there', 'were', 'calls', 'for', 'a', 'boycott', 'over', 'the', \"country's\", 'human', 'rights', 'records,', 'concerns', 'for', 'how', \"Beijing's\", 'notorious', 'smog', 'might', 'affect', 'the', 'health', 'of', 'athletes,', 'and', 'angry', 'pro-Tibet', 'protests', 'along', 'much', 'of', 'the', 'Olympic', 'Torch', 'relay.'], ngrams=['China had', 'had never', 'never hosted', 'hosted the', 'the Olympics', 'Olympics before,', 'before, and', 'and in', 'in the', 'the run-up', 'run-up to', 'to the', 'the 2008', '2008 Games', 'Games --', '-- held', 'held under', 'under the', 'the slogan', 'slogan \"One', '\"One World,', 'World, One', 'One Dream\"', 'Dream\" --', '-- there', 'there were', 'were calls', 'calls for', 'for a', 'a boycott', 'boycott over', 'over the', \"the country's\", \"country's human\", 'human rights', 'rights records,', 'records, concerns', 'concerns for', 'for how', \"how Beijing's\", \"Beijing's notorious\", 'notorious smog', 'smog might', 'might affect', 'affect the', 'the health', 'health of', 'of athletes,', 'athletes, and', 'and angry', 'angry pro-Tibet', 'pro-Tibet protests', 'protests along', 'along much', 'much of', 'of the', 'the Olympic', 'Olympic Torch', 'Torch relay.'], rawFeatures=SparseVector(50, {0: 1.0, 1: 1.0, 2: 1.0, 3: 4.0, 4: 1.0, 5: 1.0, 6: 2.0, 8: 2.0, 9: 2.0, 10: 1.0, 12: 1.0, 15: 2.0, 16: 1.0, 18: 3.0, 20: 1.0, 22: 2.0, 23: 1.0, 24: 2.0, 26: 1.0, 27: 5.0, 28: 1.0, 29: 1.0, 31: 1.0, 32: 4.0, 35: 1.0, 36: 2.0, 37: 1.0, 38: 2.0, 40: 1.0, 41: 1.0, 42: 1.0, 43: 1.0, 44: 1.0, 47: 2.0, 48: 1.0, 49: 3.0}), features=SparseVector(50, {0: 0.0, 1: 0.1823, 2: 0.1823, 3: 2.7726, 4: 0.0, 5: 0.1823, 6: 0.3646, 8: 0.3646, 9: 0.8109, 10: 0.0, 12: 0.1823, 15: 0.8109, 16: 0.0, 18: 0.0, 20: 0.4055, 22: 0.0, 23: 0.0, 24: 0.8109, 26: 0.6931, 27: 0.0, 28: 0.0, 29: 0.1823, 31: 0.0, 32: 0.7293, 35: 0.1823, 36: 0.0, 37: 0.4055, 38: 0.3646, 40: 0.0, 41: 0.0, 42: 0.0, 43: 0.1823, 44: 0.1823, 47: 0.3646, 48: 0.0, 49: 0.547}))\n",
            "(50,[0,1,2,3,4,5,6,8,9,10,12,15,16,18,20,22,23,24,26,27,28,29,31,32,35,36,37,38,40,41,42,43,44,47,48,49],[1.0,1.0,1.0,4.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,2.0,1.0,3.0,1.0,2.0,1.0,2.0,1.0,5.0,1.0,1.0,1.0,4.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0])\n",
            "Row(label=0.2, document=['A', 'homeless', 'man', 'risked', 'his', 'life', 'to', 'save', 'several', 'cats', 'and', 'dogs', 'trapped', 'at', 'an', 'Atlanta', 'animal', 'shelter', 'after', 'it', 'caught', 'fire,', 'the', \"facility's\", 'founder', 'said.Keith', 'Walker,', '53,', 'rushed', 'into', 'the', 'W-Underdogs', 'shelter', 'on', 'December', '18', 'after', 'a', 'fire', 'engulfed', 'its', 'kitchen.', '\"I', 'was', 'nervous', 'as', 'hell,', \"I'm\", 'not', 'going', 'to', 'lie.', 'I', 'was', 'really', 'scared', 'to', 'go', 'in', 'there', 'with', 'all', 'that', 'smoke.', 'But', 'God', 'put', 'me', 'there', 'to', 'save', 'those', 'animals,\"', 'Walker', 'told', 'CNN.', '\"If', 'you', 'love', 'a', 'dog,', 'you', 'can', 'love', 'anyone', 'in', 'the', 'world.', 'My', 'dog', 'is', 'my', 'best', 'friend,', 'and', 'I', \"wouldn't\", 'be', 'here', 'without', 'him,', 'so', 'I', 'knew', 'I', 'had', 'to', 'save', 'all', 'those', 'other', 'dogs.\"'], ngrams=['A homeless', 'homeless man', 'man risked', 'risked his', 'his life', 'life to', 'to save', 'save several', 'several cats', 'cats and', 'and dogs', 'dogs trapped', 'trapped at', 'at an', 'an Atlanta', 'Atlanta animal', 'animal shelter', 'shelter after', 'after it', 'it caught', 'caught fire,', 'fire, the', \"the facility's\", \"facility's founder\", 'founder said.Keith', 'said.Keith Walker,', 'Walker, 53,', '53, rushed', 'rushed into', 'into the', 'the W-Underdogs', 'W-Underdogs shelter', 'shelter on', 'on December', 'December 18', '18 after', 'after a', 'a fire', 'fire engulfed', 'engulfed its', 'its kitchen.', 'kitchen. \"I', '\"I was', 'was nervous', 'nervous as', 'as hell,', \"hell, I'm\", \"I'm not\", 'not going', 'going to', 'to lie.', 'lie. I', 'I was', 'was really', 'really scared', 'scared to', 'to go', 'go in', 'in there', 'there with', 'with all', 'all that', 'that smoke.', 'smoke. But', 'But God', 'God put', 'put me', 'me there', 'there to', 'to save', 'save those', 'those animals,\"', 'animals,\" Walker', 'Walker told', 'told CNN.', 'CNN. \"If', '\"If you', 'you love', 'love a', 'a dog,', 'dog, you', 'you can', 'can love', 'love anyone', 'anyone in', 'in the', 'the world.', 'world. My', 'My dog', 'dog is', 'is my', 'my best', 'best friend,', 'friend, and', 'and I', \"I wouldn't\", \"wouldn't be\", 'be here', 'here without', 'without him,', 'him, so', 'so I', 'I knew', 'knew I', 'I had', 'had to', 'to save', 'save all', 'all those', 'those other', 'other dogs.\"'], rawFeatures=SparseVector(50, {0: 2.0, 1: 3.0, 2: 3.0, 4: 5.0, 5: 3.0, 6: 4.0, 7: 5.0, 8: 6.0, 10: 3.0, 11: 3.0, 12: 1.0, 14: 2.0, 15: 5.0, 16: 1.0, 17: 1.0, 18: 3.0, 19: 2.0, 20: 1.0, 21: 2.0, 22: 4.0, 23: 1.0, 24: 5.0, 25: 2.0, 26: 3.0, 27: 2.0, 28: 1.0, 29: 1.0, 30: 4.0, 31: 3.0, 32: 2.0, 33: 1.0, 35: 6.0, 36: 2.0, 37: 2.0, 40: 1.0, 41: 2.0, 42: 4.0, 44: 2.0, 46: 3.0, 47: 2.0, 48: 2.0, 49: 1.0}), features=SparseVector(50, {0: 0.0, 1: 0.547, 2: 0.547, 4: 0.0, 5: 0.547, 6: 0.7293, 7: 2.0273, 8: 1.0939, 10: 0.0, 11: 0.547, 12: 0.1823, 14: 0.8109, 15: 2.0273, 16: 0.0, 17: 0.4055, 18: 0.0, 19: 0.8109, 20: 0.4055, 21: 0.8109, 22: 0.0, 23: 0.0, 24: 2.0273, 25: 0.8109, 26: 2.0794, 27: 0.0, 28: 0.0, 29: 0.1823, 30: 1.6219, 31: 0.0, 32: 0.3646, 33: 0.1823, 35: 1.0939, 36: 0.0, 37: 0.8109, 40: 0.0, 41: 0.0, 42: 0.0, 44: 0.3646, 46: 0.547, 47: 0.3646, 48: 0.0, 49: 0.1823}))\n",
            "(50,[0,1,2,4,5,6,7,8,10,11,12,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,35,36,37,40,41,42,44,46,47,48,49],[2.0,3.0,3.0,5.0,3.0,4.0,5.0,6.0,3.0,3.0,1.0,2.0,5.0,1.0,1.0,3.0,2.0,1.0,2.0,4.0,1.0,5.0,2.0,3.0,2.0,1.0,1.0,4.0,3.0,2.0,1.0,6.0,2.0,2.0,1.0,2.0,4.0,2.0,3.0,2.0,2.0,1.0])\n",
            "Row(label=0.3, document=['Hamlin', 'previously', 'knew', 'Walker,', 'who', 'has', 'been', 'homeless', 'since', 'he', 'was', '13', 'years', 'old,', 'because', 'she', 'lets', 'him', 'keep', 'his', 'dog,', 'a', 'pitbull', 'named', 'Bravo,', 'at', 'the', 'shelter', 'every', 'night.', 'Walker', 'was', 'on', 'his', 'way', 'to', 'pick', 'up', 'Bravo', 'and', 'take', 'him', 'for', 'a', 'walk', 'when', 'he', 'saw', 'the', 'fire.', 'He', 'was', 'able', 'to', 'rescue', 'all', 'the', 'animals', '--', 'six', 'dogs', 'and', '10', 'cats.', '\"I', \"can't\", 'thank', 'him', 'enough', 'for', 'saving', 'my', 'animals,\"', 'Hamlin', 'said.', '\"I\\'m', 'still', 'in', 'disbelief,', 'because', \"I've\", 'been', 'around', 'a', 'fire', 'and', 'I', 'know', 'how', 'fast', 'they', 'flare', 'up.', 'He', 'is', 'my', 'hero.\"'], ngrams=['Hamlin previously', 'previously knew', 'knew Walker,', 'Walker, who', 'who has', 'has been', 'been homeless', 'homeless since', 'since he', 'he was', 'was 13', '13 years', 'years old,', 'old, because', 'because she', 'she lets', 'lets him', 'him keep', 'keep his', 'his dog,', 'dog, a', 'a pitbull', 'pitbull named', 'named Bravo,', 'Bravo, at', 'at the', 'the shelter', 'shelter every', 'every night.', 'night. Walker', 'Walker was', 'was on', 'on his', 'his way', 'way to', 'to pick', 'pick up', 'up Bravo', 'Bravo and', 'and take', 'take him', 'him for', 'for a', 'a walk', 'walk when', 'when he', 'he saw', 'saw the', 'the fire.', 'fire. He', 'He was', 'was able', 'able to', 'to rescue', 'rescue all', 'all the', 'the animals', 'animals --', '-- six', 'six dogs', 'dogs and', 'and 10', '10 cats.', 'cats. \"I', '\"I can\\'t', \"can't thank\", 'thank him', 'him enough', 'enough for', 'for saving', 'saving my', 'my animals,\"', 'animals,\" Hamlin', 'Hamlin said.', 'said. \"I\\'m', '\"I\\'m still', 'still in', 'in disbelief,', 'disbelief, because', \"because I've\", \"I've been\", 'been around', 'around a', 'a fire', 'fire and', 'and I', 'I know', 'know how', 'how fast', 'fast they', 'they flare', 'flare up.', 'up. He', 'He is', 'is my', 'my hero.\"'], rawFeatures=SparseVector(50, {0: 2.0, 2: 1.0, 3: 2.0, 4: 1.0, 5: 1.0, 7: 2.0, 9: 5.0, 10: 2.0, 11: 3.0, 12: 3.0, 13: 1.0, 14: 2.0, 15: 6.0, 16: 2.0, 17: 2.0, 18: 1.0, 19: 3.0, 21: 2.0, 22: 2.0, 23: 1.0, 24: 1.0, 25: 4.0, 27: 1.0, 28: 2.0, 29: 2.0, 30: 2.0, 31: 4.0, 32: 2.0, 33: 4.0, 35: 1.0, 36: 1.0, 37: 2.0, 38: 2.0, 39: 2.0, 40: 1.0, 41: 2.0, 42: 2.0, 43: 1.0, 45: 2.0, 46: 1.0, 47: 3.0, 48: 4.0, 49: 6.0}), features=SparseVector(50, {0: 0.0, 2: 0.1823, 3: 1.3863, 4: 0.0, 5: 0.1823, 7: 0.8109, 9: 2.0273, 10: 0.0, 11: 0.547, 12: 0.547, 13: 0.4055, 14: 0.8109, 15: 2.4328, 16: 0.0, 17: 0.8109, 18: 0.0, 19: 1.2164, 21: 0.8109, 22: 0.0, 23: 0.0, 24: 0.4055, 25: 1.6219, 27: 0.0, 28: 0.0, 29: 0.3646, 30: 0.8109, 31: 0.0, 32: 0.3646, 33: 0.7293, 35: 0.1823, 36: 0.0, 37: 0.8109, 38: 0.3646, 39: 0.8109, 40: 0.0, 41: 0.0, 42: 0.0, 43: 0.1823, 45: 0.8109, 46: 0.1823, 47: 0.547, 48: 0.0, 49: 1.0939}))\n",
            "(50,[0,2,3,4,5,7,9,10,11,12,13,14,15,16,17,18,19,21,22,23,24,25,27,28,29,30,31,32,33,35,36,37,38,39,40,41,42,43,45,46,47,48,49],[2.0,1.0,2.0,1.0,1.0,2.0,5.0,2.0,3.0,3.0,1.0,2.0,6.0,2.0,2.0,1.0,3.0,2.0,2.0,1.0,1.0,4.0,1.0,2.0,2.0,2.0,4.0,2.0,4.0,1.0,1.0,2.0,2.0,2.0,1.0,2.0,2.0,1.0,2.0,1.0,3.0,4.0,6.0])\n",
            "Row(label=0.5, document=['Andy', 'Trust', 'has', 'been', 'exporting', 'fish', 'from', 'Cornwall', 'to', 'continental', 'Europe', 'for', '20', 'years.', 'However,', 'the', 'past', 'seven', 'weeks', 'have', 'given', 'him', 'cause', 'to', 'contemplate', 'shutting', 'down', 'the', 'entire', 'European', 'operation', 'of', 'his', 'fish', 'merchants,', 'Ocean', 'Harvest.', '\"The', 'cost', 'of', 'sending', 'fish', 'into', 'Europe', 'has', 'more', 'than', 'trebled.', 'In', 'an', 'industry', 'that', 'operates', 'on', 'thin', 'profit', 'margins,', 'it', 'could', 'destroy', 'British', 'fishing,\"', 'he', 'says.'], ngrams=['Andy Trust', 'Trust has', 'has been', 'been exporting', 'exporting fish', 'fish from', 'from Cornwall', 'Cornwall to', 'to continental', 'continental Europe', 'Europe for', 'for 20', '20 years.', 'years. However,', 'However, the', 'the past', 'past seven', 'seven weeks', 'weeks have', 'have given', 'given him', 'him cause', 'cause to', 'to contemplate', 'contemplate shutting', 'shutting down', 'down the', 'the entire', 'entire European', 'European operation', 'operation of', 'of his', 'his fish', 'fish merchants,', 'merchants, Ocean', 'Ocean Harvest.', 'Harvest. \"The', '\"The cost', 'cost of', 'of sending', 'sending fish', 'fish into', 'into Europe', 'Europe has', 'has more', 'more than', 'than trebled.', 'trebled. In', 'In an', 'an industry', 'industry that', 'that operates', 'operates on', 'on thin', 'thin profit', 'profit margins,', 'margins, it', 'it could', 'could destroy', 'destroy British', 'British fishing,\"', 'fishing,\" he', 'he says.'], rawFeatures=SparseVector(50, {0: 1.0, 1: 1.0, 2: 1.0, 4: 1.0, 6: 1.0, 8: 1.0, 9: 4.0, 10: 2.0, 11: 2.0, 12: 1.0, 13: 1.0, 16: 2.0, 17: 1.0, 18: 1.0, 20: 1.0, 21: 1.0, 22: 1.0, 23: 2.0, 27: 3.0, 28: 3.0, 30: 1.0, 31: 1.0, 32: 4.0, 33: 2.0, 34: 1.0, 36: 3.0, 38: 3.0, 39: 5.0, 40: 1.0, 41: 1.0, 42: 2.0, 43: 1.0, 44: 1.0, 45: 2.0, 46: 1.0, 48: 2.0, 49: 1.0}), features=SparseVector(50, {0: 0.0, 1: 0.1823, 2: 0.1823, 4: 0.0, 6: 0.1823, 8: 0.1823, 9: 1.6219, 10: 0.0, 11: 0.3646, 12: 0.1823, 13: 0.4055, 16: 0.0, 17: 0.4055, 18: 0.0, 20: 0.4055, 21: 0.4055, 22: 0.0, 23: 0.0, 27: 0.0, 28: 0.0, 30: 0.4055, 31: 0.0, 32: 0.7293, 33: 0.3646, 34: 0.6931, 36: 0.0, 38: 0.547, 39: 2.0273, 40: 0.0, 41: 0.0, 42: 0.0, 43: 0.1823, 44: 0.1823, 45: 0.8109, 46: 0.1823, 48: 0.0, 49: 0.1823}))\n",
            "(50,[0,1,2,4,6,8,9,10,11,12,13,16,17,18,20,21,22,23,27,28,30,31,32,33,34,36,38,39,40,41,42,43,44,45,46,48,49],[1.0,1.0,1.0,1.0,1.0,1.0,4.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,3.0,3.0,1.0,1.0,4.0,2.0,1.0,3.0,3.0,5.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}